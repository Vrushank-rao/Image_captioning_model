{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom torch import nn,autograd\nfrom torchvision import transforms\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-02-28T06:21:27.790485Z","iopub.execute_input":"2023-02-28T06:21:27.790970Z","iopub.status.idle":"2023-02-28T06:21:30.740988Z","shell.execute_reply.started":"2023-02-28T06:21:27.790927Z","shell.execute_reply":"2023-02-28T06:21:30.739584Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tensorflow.keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:30.743454Z","iopub.execute_input":"2023-02-28T06:21:30.744382Z","iopub.status.idle":"2023-02-28T06:21:40.896907Z","shell.execute_reply.started":"2023-02-28T06:21:30.744324Z","shell.execute_reply":"2023-02-28T06:21:40.895387Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"os.listdir('../input/flickr-image-dataset/flickr30k_images')","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:40.898833Z","iopub.execute_input":"2023-02-28T06:21:40.899884Z","iopub.status.idle":"2023-02-28T06:21:40.922098Z","shell.execute_reply.started":"2023-02-28T06:21:40.899824Z","shell.execute_reply":"2023-02-28T06:21:40.920817Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['flickr30k_images', 'results.csv']"},"metadata":{}}]},{"cell_type":"code","source":"metadata = pd.read_csv('../input/flickr-image-dataset/flickr30k_images/results.csv',delimiter='|',engine='python')\nmetadata = metadata.dropna()\nis_NaN = metadata.isnull()\nrow_has_NaN = is_NaN.any(axis=1)\nrows_with_NaN = metadata[row_has_NaN]\nprint(rows_with_NaN)\nmetadata.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:40.924709Z","iopub.execute_input":"2023-02-28T06:21:40.925113Z","iopub.status.idle":"2023-02-28T06:21:41.876796Z","shell.execute_reply.started":"2023-02-28T06:21:40.925066Z","shell.execute_reply":"2023-02-28T06:21:41.875371Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Empty DataFrame\nColumns: [image_name,  comment_number,  comment]\nIndex: []\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       image_name  comment_number  \\\n0  1000092795.jpg               0   \n1  1000092795.jpg               1   \n2  1000092795.jpg               2   \n3  1000092795.jpg               3   \n4  1000092795.jpg               4   \n\n                                             comment  \n0   Two young guys with shaggy hair look at their...  \n1   Two young , White males are outside near many...  \n2   Two men in green shirts are standing in a yard .  \n3       A man in a blue shirt standing in a garden .  \n4            Two friends enjoy time spent together .  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>comment_number</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000092795.jpg</td>\n      <td>0</td>\n      <td>Two young guys with shaggy hair look at their...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000092795.jpg</td>\n      <td>1</td>\n      <td>Two young , White males are outside near many...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000092795.jpg</td>\n      <td>2</td>\n      <td>Two men in green shirts are standing in a yard .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000092795.jpg</td>\n      <td>3</td>\n      <td>A man in a blue shirt standing in a garden .</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000092795.jpg</td>\n      <td>4</td>\n      <td>Two friends enjoy time spent together .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def load_image(name):\n    img = image.load_img(name,target_size=(128,128,3))\n    img = image.img_to_array(img)\n#     img = img/255\n    \n    #plt.imshow(img)\n#     img = np.reshape(img,(224*224*3))\n    return img.T","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:41.878334Z","iopub.execute_input":"2023-02-28T06:21:41.879331Z","iopub.status.idle":"2023-02-28T06:21:41.885812Z","shell.execute_reply.started":"2023-02-28T06:21:41.879290Z","shell.execute_reply":"2023-02-28T06:21:41.884227Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dict_ = {}\nfor index,row in metadata.iterrows():\n    if row['image_name'] in dict_:\n        dict_[row['image_name']].append('<SOS>'+row[' comment']+'<EOS>')\n    else:\n        dict_[row['image_name']]=['<SOS>'+row[' comment']+'<EOS>']\nprint(len(dict_))","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:41.887923Z","iopub.execute_input":"2023-02-28T06:21:41.888398Z","iopub.status.idle":"2023-02-28T06:21:51.972236Z","shell.execute_reply.started":"2023-02-28T06:21:41.888356Z","shell.execute_reply":"2023-02-28T06:21:51.970798Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"31783\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize(x):\n    \"\"\"\n    Tokenize x\n    :param x: List of sentences/strings to be tokenized\n    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n    \"\"\"\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(x)\n    t=tokenizer.texts_to_sequences(x)\n    # TODO: Implement\n    return t, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:51.974154Z","iopub.execute_input":"2023-02-28T06:21:51.974655Z","iopub.status.idle":"2023-02-28T06:21:51.981474Z","shell.execute_reply.started":"2023-02-28T06:21:51.974612Z","shell.execute_reply":"2023-02-28T06:21:51.979866Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def pad(x, length=None):\n    \"\"\"\n    Pad x\n    :param x: List of sequences.\n    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n    :return: Padded numpy array of sequences\n    \"\"\"\n    padding=pad_sequences(x,padding='post',maxlen=length)\n    return padding\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:51.982946Z","iopub.execute_input":"2023-02-28T06:21:51.983285Z","iopub.status.idle":"2023-02-28T06:21:52.000510Z","shell.execute_reply.started":"2023-02-28T06:21:51.983250Z","shell.execute_reply":"2023-02-28T06:21:51.999082Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess(sentences):\n    text_tokenized, text_tokenizer = tokenize(sentences)\n    text_pad = pad(text_tokenized)\n    return text_pad, text_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:52.002462Z","iopub.execute_input":"2023-02-28T06:21:52.002890Z","iopub.status.idle":"2023-02-28T06:21:52.017627Z","shell.execute_reply.started":"2023-02-28T06:21:52.002852Z","shell.execute_reply":"2023-02-28T06:21:52.016353Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"loc = '../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\nimage_arr = []\nsentence_arr = []\nfor ind in range(20000):\n    if ind % 2 != 0:\n        continue\n    image_location = (metadata.iloc[ind,:]['image_name'])\n    sentence = dict_[image_location]\n    \n    \n    image_arr.append(load_image(loc+str(image_location)) )\n    sentence_arr.extend(sentence) #append all 5 captions for that image\n    \n        \nImages =  np.array(image_arr)","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:52.021680Z","iopub.execute_input":"2023-02-28T06:21:52.022676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sentence , token_Sentence = preprocess(sentence_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Images =  np.array(image_arr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Sentence.shape,Images.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom architecture","metadata":{}},{"cell_type":"code","source":"cuda0 = torch.device('cuda:0')\ncuda1 = torch.device('cuda:1')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.device else \"cpu\")\n# device='cpu'\nvocab_size=len(token_Sentence.word_index)+1\nembedding_size = 512\nbatch_size=1\nmax_len_t = len(Sentence[0])\npatch_size=16\nimg_size=128\nnum_patches = (img_size * img_size) // patch_size**2 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n    \n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 512.\n    \"\"\" \n    # 2. Initialize the class with appropriate variables\n    def __init__(self, \n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=512):\n        super().__init__()\n        \n        self.patch_size = patch_size\n        \n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n#         self.flatten = nn.Flatten()\n        \n        self.flatten = nn.Flatten(start_dim=2,end_dim=3)# only flatten the feature map dimensions into a single vector\n\n    # 5. Define the forward method \n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n        \n        # Perform the forward pass \n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched) \n#         print(x_flattened.shape)\n#         print(x_flattened.permute(2,0,1).shape)\n        # 6. Make sure the output shape has the right order \n        return x_flattened.permute(2,0,1) #.permute(0,2,1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class encoder(nn.Module):\n    def __init__(self, img_size=img_size,embedding_dim=512,patch_size=16,num_channels=3):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim).to(cuda0)\n\n        self.transformer_encoder = nn.TransformerEncoder(\n            encoder_layer=nn.TransformerEncoderLayer(d_model=512,\n                                                     nhead=4,\n                                                     dim_feedforward=2048,\n                                                     activation=\"gelu\",\n#                                                      batch_first=True,\n#                                                      norm_first=True,\n                                                     device=cuda0), # Create a single Transformer Encoder Layer\n                                                     num_layers=2).to(cuda0)\n        #creating positional embedding\n\n        num_patches = (img_size * img_size) // patch_size**2 \n        self.positional_embedding = nn.Parameter(torch.randn(num_patches,batch_size,embedding_dim)).to(cuda0)\n        \n        self.embedding_dropout = nn.Dropout(p=0.1).to(cuda0)\n    def forward(self,images):\n        \n        batches = images.shape[0]\n        x = self.patch_embedding(images)\n#         print('patch embed',x.shape)\n#         x = x[np.newaxis,0:].T\n#         print('position embed',self.positional_embedding.shape)\n        x = self.positional_embedding + x\n        x.requires_grad_()\n        trans_encoder = self.transformer_encoder(x)\n        return trans_encoder#(1,0,2) 2 is the last index , 0 is the batchsize","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = encoder()\n        self.trg_word_embedding = nn.Embedding(vocab_size, embedding_size)\n        self.trg_position_embedding = nn.Embedding(max_len_t, embedding_size)\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer=nn.TransformerDecoderLayer(d_model=embedding_size,\n                                                     nhead=4,\n#                                                      batch_first=True,\n                                                     activation='gelu',\n#                                                      norm_first=True,\n                                                     device=cuda0), \n                                                     num_layers=2).to(cuda0)\n        self.out = nn.Linear(embedding_size, vocab_size)\n        self.dropout = nn.Dropout(0.2).to(cuda0)\n    \n    def mask(self,sz):\n        \n        mask = (torch.triu(torch.ones(sz[0], sz[0])) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')). \\\n        masked_fill(mask == 1, float(0.0))\n        return mask\n  \n        \n    def forward(self,img,trg):\n#         print(trg.shape)\n#         print(type(trg))\n#         print(self.trg_word_embedding)\n        trg=trg.type(torch.int64)\n        trg_seq_length,N = trg.shape\n#         print('in gene',trg_seq_length)\n        trg_positions = (\n            torch.arange(0, trg_seq_length)\n            .unsqueeze(1)\n            .expand(trg_seq_length,N)\n            \n        ).to(cuda0)\n#         print(trg_positions)\n\n        embed_trg = self.dropout(\n                self.trg_word_embedding(trg) +\n                 self.trg_position_embedding(trg_positions)\n            )\n        in_features = self.encoder(img)\n#         print('the shape of mat from encoder is ',in_features.shape,\n#               '\\n embeded trg',embed_trg.shape)\n        in_features.requires_grad_()\n        trg_mask = self.mask(trg.shape).to(cuda0)\n        logits = self.transformer_decoder(embed_trg,in_features,tgt_mask=trg_mask) #in_features is from the encoder\n        \n        logits = self.out(logits)\n#         print(logits.shape)\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=1\ns = torch.from_numpy(Sentence[(i-1)*batch_size : (i-1)*1 + batch_size]).type(torch.long).T.shape\n# f=(Images[np.newaxis,0:]).shape\nf = torch.from_numpy(Images[0:1]).to(torch.long).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s,f","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=generator(),input_size=(f,s))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class discriminator(nn.Module):\n    def __init__(self,img_size=img_size,embedding_dim=512,patch_size=16,num_channels=3):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim).to(cuda1)\n        self.transformer_decoder = nn.TransformerDecoder(\n                decoder_layer=nn.TransformerDecoderLayer(d_model=embedding_size,\n                                                         nhead=4,\n                                                         activation='gelu',\n                                                         device=cuda1), \n                                                         num_layers=2).to(cuda1)\n        self.trg_word_embedding = nn.Embedding(vocab_size, embedding_size)\n        self.trg_position_embedding = nn.Embedding(max_len_t, embedding_size)\n        \n        self.out = nn.Sequential(\n            nn.Flatten(start_dim=0,end_dim=-1),\n            nn.Linear(40960,1),\n            nn.Sigmoid())\n       \n        self.dropout = nn.Dropout(0.2).to(cuda1)\n        \n    def forward(self,img,trg):\n        \n        trg=trg.type(torch.int64)\n#         print(trg.shape)\n        trg_seq_length,N = trg.shape\n#         print('in dis',trg_seq_length)\n        trg_positions = (\n            torch.arange(0, trg_seq_length)\n            .unsqueeze(1)\n            .expand(trg_seq_length,N)\n            \n        ).to(cuda1)\n#         print(trg_positions)\n        \n\n        embed_trg = self.dropout(\n                self.trg_word_embedding(trg) +\n                 self.trg_position_embedding(trg_positions)\n            )\n#         print(embed_trg.shape)\n        x = self.patch_embedding(img)\n        \n        prob = self.out(self.transformer_decoder(embed_trg,x))\n        return prob # 0/1 for fake and real","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:17:55.542937Z","iopub.execute_input":"2023-02-27T11:17:55.543328Z","iopub.status.idle":"2023-02-27T11:17:55.555880Z","shell.execute_reply.started":"2023-02-27T11:17:55.543292Z","shell.execute_reply":"2023-02-27T11:17:55.554821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=discriminator(),input_size=(f,s))","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:17:56.248087Z","iopub.execute_input":"2023-02-27T11:17:56.248860Z","iopub.status.idle":"2023-02-27T11:18:00.126384Z","shell.execute_reply.started":"2023-02-27T11:17:56.248802Z","shell.execute_reply":"2023-02-27T11:18:00.125401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## train the gan model along with the encoder","metadata":{}},{"cell_type":"code","source":"from torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:49:38.909004Z","iopub.execute_input":"2023-02-27T09:49:38.909678Z","iopub.status.idle":"2023-02-27T09:49:38.914633Z","shell.execute_reply.started":"2023-02-27T09:49:38.909635Z","shell.execute_reply":"2023-02-27T09:49:38.913570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_batch(src, tar , batchsize , i):\n    \n\n    src=  src[(i-1)*batchsize : (i-1)*batchsize + batchsize]\n    tar =tar[(1-1)*batchsize : (1-1)*batchsize + batchsize]\n#     print(tar.shape)\n    return torch.tensor(src).float(),torch.tensor(tar).T.long()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:03.010086Z","iopub.execute_input":"2023-02-27T11:18:03.010900Z","iopub.status.idle":"2023-02-27T11:18:03.017514Z","shell.execute_reply.started":"2023-02-27T11:18:03.010859Z","shell.execute_reply":"2023-02-27T11:18:03.016288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = generator().to(cuda0)\n\noptimizer = torch.optim.SGD(c.parameters(),lr=0.01, momentum=0.9,nesterov=True)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.1, patience=10, verbose=True,\n)\npad_idx = 0\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:05.439695Z","iopub.execute_input":"2023-02-27T11:18:05.440372Z","iopub.status.idle":"2023-02-27T11:18:05.561261Z","shell.execute_reply.started":"2023-02-27T11:18:05.440329Z","shell.execute_reply":"2023-02-27T11:18:05.560193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = discriminator().to(cuda1)\nd_optimizer = torch.optim.Adam(d.parameters(),lr=0.1)\nd_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\nd_optimizer, factor=0.1, patience=10, verbose=True)\nd_criterioin = nn.BCELoss().to(cuda1)   #therewas no .to\n","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:05.961173Z","iopub.execute_input":"2023-02-27T11:18:05.961678Z","iopub.status.idle":"2023-02-27T11:18:06.044887Z","shell.execute_reply.started":"2023-02-27T11:18:05.961641Z","shell.execute_reply":"2023-02-27T11:18:06.043780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_device = next(d.parameters()).device\nc_device = next(c.parameters()).device","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:07.624083Z","iopub.execute_input":"2023-02-27T11:18:07.624503Z","iopub.status.idle":"2023-02-27T11:18:07.630210Z","shell.execute_reply.started":"2023-02-27T11:18:07.624469Z","shell.execute_reply":"2023-02-27T11:18:07.629030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient penality","metadata":{}},{"cell_type":"code","source":"# def gradient_penalty(D, xr, xf):\n    \n    \n#     # [b, 1]\n#     t = torch.rand(batch_size, 1).to(device)\n#     # [b, 1] => [b, 2]  broadcasting so t is the same for x1 and x2\n#     t = t.expand_as(xr)\n#     # interpolation\n#     mid = t * xr + (1 - t) * xf\n#     # set it to require grad info\n#     mid.requires_grad_()    \n#     pred = D(mid,t)\n#     grads = autograd.grad(outputs=pred, inputs=mid,\n#                           grad_outputs=torch.ones_like(pred),\n#                           create_graph=True, retain_graph=True, only_inputs=True)[0] \n#     gp = torch.pow(grads.norm(2, dim=1) - 1, 2).mean()\n#     return gp","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:49:39.273733Z","iopub.execute_input":"2023-02-27T09:49:39.274303Z","iopub.status.idle":"2023-02-27T09:49:39.284866Z","shell.execute_reply.started":"2023-02-27T09:49:39.274274Z","shell.execute_reply":"2023-02-27T09:49:39.283644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real = torch.ones(1).to(d_device)\nfake = torch.zeros(1).to(d_device)\nreal","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:13.865364Z","iopub.execute_input":"2023-02-27T11:18:13.866089Z","iopub.status.idle":"2023-02-27T11:18:13.929211Z","shell.execute_reply.started":"2023-02-27T11:18:13.866052Z","shell.execute_reply":"2023-02-27T11:18:13.928076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\nimport gc\ndef free_gpu_cache():                \n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:15.031502Z","iopub.execute_input":"2023-02-27T11:18:15.032203Z","iopub.status.idle":"2023-02-27T11:18:33.042312Z","shell.execute_reply.started":"2023-02-27T11:18:15.032164Z","shell.execute_reply":"2023-02-27T11:18:33.041012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\nimport random\ndef train(i,j):\n    \n    c.train()\n    start = i\n    end = j\n    start_time = time.time()\n    \n    for rep in range(5): \n        total_loss = 0\n        for i in range(start,end):\n            '''sentence is already preprocessed\n            the image is normal image and encoder does the image embedding \n            then the generator takes the image embedding from the encoder and predicts the caption\n            then discriminator's job is to correct the generator to learn the caption from the image.\n            The sentence has to similar to any-one of the 5 sentences in the dataset.\n            This caption need not be same but it is expected to be 95% similar to atleast one of them\n            '''\n\n            \n            s = Sentence[(i-1)*5:((i-1)*5)+5]\n\n            random.shuffle(s)\n\n            src,tar = create_batch(Images,s, batch_size , i)\n\n            src=src.detach().clone().to(d_device)\n            tar=tar.detach().clone().to(d_device)\n            src1=src.detach().clone().to(c_device)\n            tar1=tar.detach().clone().to(c_device)\n\n            for _ in range(3): # passing the same text 10 times\n                d.train()\n                pred_r=d(src,tar)\n\n                lossr = -d_criterioin(pred_r,real).mean()\n\n                # 1.2 train on fake data\n                \n                xf = c(src1,tar1).detach()  # gradient would not be passed down\n                val, ind = torch.max(xf.view(-1, xf.shape[2]), 1)\n                ind = torch.tensor(ind).reshape(80,1).to(d_device)\n\n                predf = d(src,ind) \n                \n                # min predf\n                lossf =d_criterioin(predf,fake).mean()\n                # 1.3 gradient penalty\n#                 gp = gradient_penalty(d, tar, xf.detach())            # aggregate all\n                loss_D = lossr + lossf           # optimize\n\n                d_optimizer.zero_grad()\n                loss_D.backward()\n                torch.nn.utils.clip_grad_norm_(d.parameters(), 0.5)\n                d_optimizer.step()\n                free_gpu_cache()\n            \n\n\n\n            # 2. train caption model\n\n            xf = c(src1,tar1)\n\n            loss_G=criterion(xf.view(-1, xf.shape[2]), tar1.reshape(-1))\n    #         print('after loss',xf.shape,'vs',tar.reshape(-1).shape)\n            optimizer.zero_grad()\n            loss_G.backward()\n            torch.nn.utils.clip_grad_norm_(c.parameters(), 0.5)\n            optimizer.step()\n        #detach the inputs\n            src.detach()\n            tar.detach()\n            src1.detach()\n            tar1.detach()\n            total_loss+=loss_G.item()\n \n            if i % 500==0:\n\n                elapsed = time.time() - start_time\n                print('rep = ',rep+1,'  loss = ',loss_G,\n                      '    time elapesed ',round(elapsed  / 200,2))\n\n                start_time = time.time()\n        print('*'*50)\n    return total_loss\n    \n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:33.044960Z","iopub.execute_input":"2023-02-27T11:18:33.045613Z","iopub.status.idle":"2023-02-27T11:18:33.062766Z","shell.execute_reply.started":"2023-02-27T11:18:33.045565Z","shell.execute_reply":"2023-02-27T11:18:33.061671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nstart,end = 1,1999\nfor epoch in range(1, 6):\n    epoch_start_time = time.time()\n    loss = train(start,end)\n    start+=end\n    end+=1999\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | Training loss {:5.2f} | '\n          .format(epoch, (time.time() - epoch_start_time),loss))","metadata":{"execution":{"iopub.status.busy":"2023-02-27T11:18:33.064295Z","iopub.execute_input":"2023-02-27T11:18:33.065203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation ","metadata":{}},{"cell_type":"markdown","source":"### For diversity lets use n-gram diversity and for similarity lets use BLEU score and CIDEr","metadata":{}},{"cell_type":"code","source":"# ! mkdir saved_models\n# PATH = f'saved_models/{c}'\n# torch.save(c.state_dict(), PATH)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-28T06:21:05.848929Z","iopub.execute_input":"2023-02-28T06:21:05.849335Z","iopub.status.idle":"2023-02-28T06:21:07.052837Z","shell.execute_reply.started":"2023-02-28T06:21:05.849301Z","shell.execute_reply":"2023-02-28T06:21:07.051034Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/2471685588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' mkdir saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'saved_models/{model}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# loading the model\n\n\n# c = generator()\n# model_state_dict = torch.load(PATH) # loading the dictionary object\n# c.load_state_dict(model_state_dict) # load_state_dict() function takes a dictionary object, NOT a path to a saved object\n# c.eval() # since we need to use the model for inference\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_image(name):\n    img = image.load_img(name,target_size=(512,512,3))\n    img = image.img_to_array(img)\n    img = img/255\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.076264Z","iopub.status.idle":"2023-02-27T09:55:33.076978Z","shell.execute_reply.started":"2023-02-27T09:55:33.076717Z","shell.execute_reply":"2023-02-27T09:55:33.076743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\ndef evaluate(index):\n    ref = ' '.join(metadata[' comment'][index].split())\n    \n    image_location, sent = metadata.iloc[index,0],metadata.iloc[index,2]\n    other = dict_[image_location]\n    image_arr = []\n    img = load_image(loc+str(image_location))\n    image_arr.append(img)\n    img_arr = np.array(image_arr)\n    sentence = []\n    sentence.append(sent)\n    sentence[0] = '<SOS> '+sentence[0]+'<EOS>'\n    sentence = pad(token_Sentence.texts_to_sequences(sentence) , length = max_len_t)\n    src , tar = create_batch(img_arr,sentence, 1,1)\n    src = src.to(c_device)\n    tar = tar.to(c_device)\n    c.eval()\n    output =  c(src,tar)\n    src.detach()\n    tar.detach()\n    loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n    sentence_formed = ''\n    val, ind = torch.max(output.view(-1, output.shape[2]), 1)\n    for word in ind:\n        #print('--->'+sentence_formed+'    '+str(word.item()))\n        if word.item() == 3: # EOS\n                break\n        for key, value in token_Sentence.word_index.items():\n            #print(value == word.item()) \n            if value == word.item() and value != 2: # sos\n                sentence_formed = sentence_formed + key +' '\n                break\n    display_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n#     print('loss = ',loss)\n#     print(output)\n    \n    print('Individual 1-gram: %f' % sentence_bleu(ref.split(), sentence_formed.split(), weights=(1, 0, 0, 0)))\n    print('Individual 2-gram: %f' % sentence_bleu(ref.split(), sentence_formed.split(), weights=(0, 1, 0, 0)))\n    print('Individual 3-gram: %f' % sentence_bleu(ref.split(), sentence_formed.split(), weights=(0, 0, 1, 0)))\n    print('Individual 4-gram: %f' % sentence_bleu(ref.split(), sentence_formed.split(), weights=(0, 0, 0, 1)))\n    score = sentence_bleu(ref.split(), sentence_formed.split(),weights=(0.25, 0.5, 0.15, 0.10))\n    print('Orginal ->',ref)\n    print('Bleu Score is {}'.format(score))\n    print('other original sentences are', other)\n    return sentence_formed , loss","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.079359Z","iopub.status.idle":"2023-02-27T09:55:33.080276Z","shell.execute_reply.started":"2023-02-27T09:55:33.079962Z","shell.execute_reply":"2023-02-27T09:55:33.079990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(0)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.081816Z","iopub.status.idle":"2023-02-27T09:55:33.082704Z","shell.execute_reply.started":"2023-02-27T09:55:33.082437Z","shell.execute_reply":"2023-02-27T09:55:33.082464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(10)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.084261Z","iopub.status.idle":"2023-02-27T09:55:33.085096Z","shell.execute_reply.started":"2023-02-27T09:55:33.084833Z","shell.execute_reply":"2023-02-27T09:55:33.084860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(6000)","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.086648Z","iopub.status.idle":"2023-02-27T09:55:33.087566Z","shell.execute_reply.started":"2023-02-27T09:55:33.087295Z","shell.execute_reply":"2023-02-27T09:55:33.087323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = [[1.1, 1.2, 1.8, 0.5, 5.9, -2.3],[-1.1, 1.2, 1.8, 0.5, 5.9, -2.3]]\nans = []\nfor row in a:\n    ans.append([int(a) for a in row])\nans","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.089078Z","iopub.status.idle":"2023-02-27T09:55:33.089946Z","shell.execute_reply.started":"2023-02-27T09:55:33.089653Z","shell.execute_reply":"2023-02-27T09:55:33.089679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('h')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.091532Z","iopub.status.idle":"2023-02-27T09:55:33.092374Z","shell.execute_reply.started":"2023-02-27T09:55:33.092089Z","shell.execute_reply":"2023-02-27T09:55:33.092115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('checking')","metadata":{"execution":{"iopub.status.busy":"2023-02-27T09:55:33.093862Z","iopub.status.idle":"2023-02-27T09:55:33.094705Z","shell.execute_reply.started":"2023-02-27T09:55:33.094443Z","shell.execute_reply":"2023-02-27T09:55:33.094469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"scrolled":true},"execution_count":null,"outputs":[]}]}